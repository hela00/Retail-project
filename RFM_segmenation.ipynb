{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8e6e706-037a-4476-8d0c-c5d1a8ce9a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e143d14-1c3b-4d6f-9f0a-c34b1c104d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CONFIG = {\n",
    "    'user': 'postgres',\n",
    "    'password': 'postgre',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432',\n",
    "    'database': 'sales_db'\n",
    "}\n",
    "\n",
    "\n",
    "RAW_DIR = './Sales_details'\n",
    "ARCHIVE_DIR = './data/processed'\n",
    "os.makedirs(ARCHIVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e0bb340-8b26-4ff9-a496-2dc5cdb29ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Successful !\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"Connection Successful !\")\n",
    "except Exception as e:\n",
    "    print(f\" Error connecting : {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "facf8f4a-8320-4cf3-98ce-f3b9f2279878",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \"Order_ID\", \"Customer_ID\", \"Customer_Type\", \"Product\", \"Category\", \n",
    "       \"Unit_Price\", \"Quantity\", \"Discount\", \"Total_Price\", \"Region\", \n",
    "       \"Sales_Order_Date\"\n",
    "FROM sales\n",
    "WHERE \"Sales_Order_Date\" BETWEEN '2023-01-01' AND '2024-12-31'\n",
    "\"\"\"\n",
    "data = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d9bee03-2fa8-4469-86a4-efc46f8b3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sales_Order_Date'] = pd.to_datetime(data['Sales_Order_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d6a871a-203a-49b9-aaff-eb36fc5d65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_date = data['Sales_Order_Date'].max()\n",
    "\n",
    "rfm_all = data.groupby(['Customer_ID', 'Customer_Type']).agg({\n",
    "    'Sales_Order_Date': lambda x: (latest_date - x.max()).days,\n",
    "    'Order_ID': 'nunique',\n",
    "    'Total_Price': 'sum',\n",
    "    'Region': lambda x: x.mode()[0],  # most frequent region\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7a70a3f-cb37-4c3c-a852-fcabff46971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_all.columns = ['Customer_ID', 'Customer_Type', 'Recency', 'Frequency', 'Monetary', 'Region']\n",
    "rfm_all['Monetary'] = rfm_all['Monetary'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "rfm_all['Churn_Flag'] = rfm_all['Recency'].apply(lambda x: 1 if x > 90 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a7779a7-2fe3-4093-a5c1-2ff5f3b8366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AOV\n",
    "rfm_all['AOV'] = rfm_all['Monetary'] / rfm_all['Frequency']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "479f6f25-3d66-4587-b0e0-2162a31c43e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_discount = data.groupby(['Customer_ID', 'Customer_Type'])['Discount'].mean().reset_index()\n",
    "avg_discount.columns = ['Customer_ID', 'Customer_Type', 'Avg_Discount']\n",
    "rfm_all = pd.merge(rfm_all, avg_discount, on=['Customer_ID', 'Customer_Type'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75ec3d75-1a84-42cb-b17b-e5223331aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_variety = data.groupby(['Customer_ID', 'Customer_Type'])['Product'].nunique().reset_index()\n",
    "product_variety.columns = ['Customer_ID', 'Customer_Type', 'Product_Variety']\n",
    "rfm_all = pd.merge(rfm_all, product_variety, on=['Customer_ID', 'Customer_Type'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "888bf526-a457-4709-ab28-4fa7748cc3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_category = data.groupby(['Customer_ID', 'Customer_Type'])['Category'].agg(lambda x: x.mode()[0]).reset_index()\n",
    "top_category.columns = ['Customer_ID', 'Customer_Type', 'Top_Category']\n",
    "rfm_all = pd.merge(rfm_all, top_category, on=['Customer_ID', 'Customer_Type'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eadb051d-e930-46cd-9ad3-fbeaceb883d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_interval = data.sort_values('Sales_Order_Date') \\\n",
    "    .groupby(['Customer_ID', 'Customer_Type'])['Sales_Order_Date'] \\\n",
    "    .apply(lambda x: x.diff().dt.days.mean()).reset_index(name='Avg_Purchase_Interval')\n",
    "rfm_all = pd.merge(rfm_all, purchase_interval, on=['Customer_ID', 'Customer_Type'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9b8b391-9a60-4e23-adc8-c8a4f287606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_segments = []\n",
    "plot_dir = \"./outputs/segmentation_plots\"\n",
    "os.makedirs(plot_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "96307176-0f7d-4889-b622-462e541cea1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of rfm_all: (10000, 12)\n",
      "\n",
      "Columns in rfm_all:\n",
      "- Customer_ID\n",
      "- Customer_Type\n",
      "- Recency\n",
      "- Frequency\n",
      "- Monetary\n",
      "- Region\n",
      "- Churn_Flag\n",
      "- AOV\n",
      "- Avg_Discount\n",
      "- Product_Variety\n",
      "- Top_Category\n",
      "- Avg_Purchase_Interval\n",
      "\n",
      "Region columns: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Shape of rfm_all:\", rfm_all.shape)\n",
    "print(\"\\nColumns in rfm_all:\")\n",
    "for col in rfm_all.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "\n",
    "region_cols = [col for col in rfm_all.columns if col.startswith('Region_')]\n",
    "print(\"\\nRegion columns:\", region_cols)\n",
    "\n",
    "\n",
    "if region_cols:\n",
    "    print(\"\\nSample of Region columns (first 5 rows):\")\n",
    "    print(rfm_all[region_cols].head())\n",
    "    \n",
    "    \n",
    "    print(\"\\nData types of Region columns:\")\n",
    "    print(rfm_all[region_cols].dtypes)\n",
    "    \n",
    "    \n",
    "    print(\"\\nMissing values in Region columns:\")\n",
    "    print(rfm_all[region_cols].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c715b3e4-5830-4ba2-8c4e-752760e55a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final rfm_clean shape: (10000, 12)\n",
      "Final columns: ['Customer_ID', 'Customer_Type', 'Recency', 'Frequency', 'Monetary', 'Region', 'Churn_Flag', 'AOV', 'Avg_Discount', 'Product_Variety', 'Top_Category', 'Avg_Purchase_Interval']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rfm_clean = rfm_all.copy()\n",
    "\n",
    "\n",
    "region_cols = [col for col in rfm_clean.columns if col.startswith('Region_')]\n",
    "if region_cols:\n",
    "    \n",
    "    rfm_clean['Region'] = None\n",
    "    \n",
    "    \n",
    "    for col in region_cols:\n",
    "        region_name = col.replace('Region_', '')\n",
    "        # Check if column is boolean or numeric\n",
    "        if rfm_clean[col].dtype == bool:\n",
    "            rfm_clean.loc[rfm_clean[col] == True, 'Region'] = region_name\n",
    "        else:\n",
    "            rfm_clean.loc[rfm_clean[col] == 1, 'Region'] = region_name\n",
    "    \n",
    "    \n",
    "    print(\"\\nRegion column value counts:\")\n",
    "    print(rfm_clean['Region'].value_counts())\n",
    "    print(\"Null regions:\", rfm_clean['Region'].isna().sum())\n",
    "    \n",
    "    # Now drop the one-hot columns\n",
    "    rfm_clean = rfm_clean.drop(columns=region_cols)\n",
    "\n",
    "\n",
    "cat_cols = [col for col in rfm_clean.columns if col.startswith('Top_Category_')]\n",
    "if cat_cols:\n",
    "   \n",
    "    rfm_clean['Top_Category'] = None\n",
    "    \n",
    "    \n",
    "    for col in cat_cols:\n",
    "        cat_name = col.replace('Top_Category_', '')\n",
    "        # Check if column is boolean or numeric\n",
    "        if rfm_clean[col].dtype == bool:\n",
    "            rfm_clean.loc[rfm_clean[col] == True, 'Top_Category'] = cat_name\n",
    "        else:\n",
    "            rfm_clean.loc[rfm_clean[col] == 1, 'Top_Category'] = cat_name\n",
    "    \n",
    " \n",
    "    print(\"\\nTop_Category column value counts:\")\n",
    "    print(rfm_clean['Top_Category'].value_counts())\n",
    "    print(\"Null categories:\", rfm_clean['Top_Category'].isna().sum())\n",
    "    \n",
    "    \n",
    "    rfm_clean = rfm_clean.drop(columns=cat_cols)\n",
    "\n",
    "\n",
    "print(\"\\nFinal rfm_clean shape:\", rfm_clean.shape)\n",
    "print(\"Final columns:\", rfm_clean.columns.tolist())\n",
    "\n",
    "\n",
    "rfm_all = rfm_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "39866bb3-82a4-4edf-8658-be3fb86b2d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Segmenting: B2C\n",
      "Region values mapped to: {'Baden-Württemberg': 0, 'Bayern': 1, 'Berlin': 2, 'Brandenburg': 3, 'Bremen': 4, 'Hamburg': 5, 'Hessen': 6, 'Mecklenburg-Vorpommern': 7, 'Niedersachsen': 8, 'Nordrhein-Westfalen': 9, 'Rheinland-Pfalz': 10, 'Saarland': 11, 'Sachsen': 12, 'Sachsen-Anhalt': 13, 'Schleswig-Holstein': 14, 'Thüringen': 15}\n",
      "Cluster distribution for B2C: {1: 3205, 0: 3172, 2: 60}\n",
      "\n",
      "🔍 Segmenting: B2B\n",
      "Region values mapped to: {'Baden-Württemberg': 0, 'Bayern': 1, 'Berlin': 2, 'Brandenburg': 3, 'Bremen': 4, 'Hamburg': 5, 'Hessen': 6, 'Mecklenburg-Vorpommern': 7, 'Niedersachsen': 8, 'Nordrhein-Westfalen': 9, 'Rheinland-Pfalz': 10, 'Saarland': 11, 'Sachsen': 12, 'Sachsen-Anhalt': 13, 'Schleswig-Holstein': 14, 'Thüringen': 15}\n",
      "Cluster distribution for B2B: {0: 1250, 2: 1164, 1: 1149}\n",
      "Final segmentation complete with 10000 records\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# lists to store final segments\n",
    "final_segments = []\n",
    "all_profiles = None\n",
    "\n",
    "\n",
    "for ctype in rfm_all['Customer_Type'].unique():\n",
    "    print(f\"\\n🔍 Segmenting: {ctype}\")\n",
    "    df = rfm_all[rfm_all['Customer_Type'] == ctype].copy()\n",
    "    \n",
    "    # numeric features for clustering\n",
    "    numeric_features = [\n",
    "        'Recency', 'Frequency', 'Monetary', 'AOV', 'Avg_Discount',\n",
    "        'Product_Variety', 'Avg_Purchase_Interval'\n",
    "    ]\n",
    "    \n",
    "    #only include columns that exist\n",
    "    numeric_features = [col for col in numeric_features if col in df.columns]\n",
    "    \n",
    "    #dataframe for clustering\n",
    "    X = df[numeric_features].copy()\n",
    "    \n",
    "    \n",
    "    if 'Region' in df.columns and df['Region'].notna().any():\n",
    "        encoder = LabelEncoder()\n",
    "        X['Region_encoded'] = encoder.fit_transform(df['Region'].fillna('Unknown'))\n",
    "        print(f\"Region values mapped to: {dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))}\")\n",
    "    \n",
    "    if 'Top_Category' in df.columns and df['Top_Category'].notna().any():\n",
    "        encoder = LabelEncoder()\n",
    "        X['Top_Category_encoded'] = encoder.fit_transform(df['Top_Category'].fillna('Unknown'))\n",
    "    \n",
    "    # all features to use for clustering\n",
    "    all_features = numeric_features.copy()\n",
    "    if 'Region_encoded' in X.columns:\n",
    "        all_features.append('Region_encoded')\n",
    "    if 'Top_Category_encoded' in X.columns:\n",
    "        all_features.append('Top_Category_encoded')\n",
    "    \n",
    "    # drop rows with any missing values in clustering features\n",
    "    mask = X[all_features].notna().all(axis=1)\n",
    "    X_filtered = X[mask].copy()\n",
    "    df_filtered = df[mask].copy()  \n",
    "    \n",
    "    if len(X_filtered) < 2:\n",
    "        print(f\"WARNING: Not enough data points for {ctype} after filtering NA values. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Scale features \n",
    "    X_scaled = StandardScaler().fit_transform(X_filtered[all_features])\n",
    "    \n",
    "    # Elbow Method\n",
    "    inertias = []\n",
    "    max_k = min(7, len(X_filtered))\n",
    "    for k in range(2, max_k):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(range(2, max_k), inertias, marker='o')\n",
    "    plt.title(f\"Elbow Plot - {ctype}\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Inertia\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{plot_dir}/elbow_{ctype}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Clustering\n",
    "    optimal_k = min(3, len(X_filtered) - 1)  # Ensure we don't try to create more clusters than data points\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    cluster_assignments = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    \n",
    "    df_clustered = df_filtered.copy()\n",
    "    df_clustered['Cluster'] = cluster_assignments\n",
    "    \n",
    "    # Debug info\n",
    "    print(f\"Cluster distribution for {ctype}: {df_clustered['Cluster'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # cluster profiles using original categorical values\n",
    "    cluster_profiles = df_clustered.groupby('Cluster')[numeric_features].median().reset_index()\n",
    "    \n",
    "    #  get most common value per cluster\n",
    "    if 'Region' in df.columns and df['Region'].notna().any():\n",
    "        region_mode = df_clustered.groupby('Cluster')['Region'].agg(\n",
    "            lambda x: x.mode()[0] if not x.mode().empty else None\n",
    "        ).reset_index()\n",
    "        cluster_profiles = cluster_profiles.merge(region_mode, on='Cluster')\n",
    "    \n",
    "    if 'Top_Category' in df.columns and df['Top_Category'].notna().any():\n",
    "        category_mode = df_clustered.groupby('Cluster')['Top_Category'].agg(\n",
    "            lambda x: x.mode()[0] if not x.mode().empty else None\n",
    "        ).reset_index()\n",
    "        cluster_profiles = cluster_profiles.merge(category_mode, on='Cluster')\n",
    "    \n",
    "    cluster_profiles['Customer_Type'] = ctype\n",
    "    \n",
    "   \n",
    "    if all_profiles is None:\n",
    "        all_profiles = cluster_profiles\n",
    "    else:\n",
    "        all_profiles = pd.concat([all_profiles, cluster_profiles], ignore_index=True)\n",
    "    \n",
    "   \n",
    "    df_clustered['Segment_Label'] = df_clustered['Cluster'].map(cluster_labels)\n",
    "    \n",
    "    # PCA \n",
    "    pca = PCA(n_components=2)\n",
    "    components = pca.fit_transform(X_scaled)\n",
    "    df_clustered['PCA1'], df_clustered['PCA2'] = components[:, 0], components[:, 1]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(data=df_clustered, x='PCA1', y='PCA2', hue='Cluster', palette='Set2', s=60)\n",
    "    plt.title(f\"PCA Cluster Visualization - {ctype}\")\n",
    "    plt.legend(title=\"Cluster\")\n",
    "    plt.savefig(f\"{plot_dir}/pca_{ctype}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    final_segments.append(df_clustered)\n",
    "\n",
    "# Combine all segmented data\n",
    "if final_segments:\n",
    "    final_df = pd.concat(final_segments, ignore_index=True)\n",
    "    print(f\"Final segmentation complete with {len(final_df)} records\")\n",
    "else:\n",
    "    print(\"No segments were created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e327c969-b2c8-474a-82db-f94258859a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(final_segments, ignore_index=True)\n",
    "result.to_csv('./data/processed/customer_segments_rich.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "69c89c6a-9f97-4f65-8935-6771cf15c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_profiles.to_csv('./data/processed/segment_profiles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2462007a-a1f4-421b-b1a8-cab8e120b7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_sql(\"customer_segments_RFM\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d47b6691-1948-4e5d-ae09-ac5e0417bd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_profiles.to_sql(\"segment_profiles\", engine, if_exists=\"replace\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
